
Лаборатории интеллектуальных систем ВятГУ

Для таких задач используют модели семейства Qwen, Llama и с недавних пор gemma. Размер LLM и глубину квантования можно посоветовать только исходя из понимания имеющейся видеокарты. Вариант - покупать токены у провайдеров опенсорсных LLM, но в таком случае это будет только демо вариант, т.к. выход за контур предприятия и потенциально ИБ может не пропустить. Далее. Обучение LLM на документах никто не делает - есть много исследований, которые ссылаться к тому, что это только портит модель. Грамотно обучить LLM мало кто у нас в стране может, да и железные ресурсы для этого нужны впечатляющие. Могу предложить такой вариант: отдаем вам нашу базовую RAG-систему бесплатно по соглашению, разрешающему ее неограниченное распространение и использование внутри организации, но с запретом на передачу другим юрлицам и на модификацию и использование кода или его частей. Она из коробки хорошо работает с документами, за исключением поиска по сложным таблицам, схемам и графикам. Здесь у нас скоро выйдет мультимодальный поиск, который уже умеет.  
задача сравнения и актуализации документов
### RAG
- [Архитектура RAG: полный гайд](https://habr.com/ru/companies/raft/articles/791034/)
- [Что такое RAG?](https://aws.amazon.com/ru/what-is/retrieval-augmented-generation/)
- Основная проблема RAG-подхода, на которой многие спотыкаются - это сегментация [(чанкинг)](https://towardsdatascience.com/rag-101-chunking-strategies-fdc6f6c2aaec/).
- [Создание решения RAG - Azure AI Search](https://learn.microsoft.com/ru-ru/azure/search/tutorial-rag-build-solution)

  
